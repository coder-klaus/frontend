可以通过 `readFile` 一次性读取整个文件，但存在如下问题

1. 无法精准控制读取位置 => 比如我只想读取一部分内容，而不是全部的内容
2. 性能较差 
   + 如果文件较大，一次性读取，会导致内存压力激增
   + 会阻塞后续逻辑执行，导致应用卡顿



为了解决这个问题，就需要使用 可读流 进行 分批读取

```js
// 流都是基于事件的，仅存在于 fs 模块中，不在 fs/promises 模块中
import fs from 'fs';

// 通过 createReadStream 创建一个可读流
// 参数一 => 文件路径
// 参数二 => 配置对象
// + start 从哪个字节开始读取
// + end 读取到哪个字节结束 => 是包头且包尾的 即 [start, end]
// + highWaterMark => 每次最多读取多少字节 => 默认是 64kb
const stream = fs.createReadStream('./data.txt', {
  highWaterMark: 3, // 每次最多读取 3 个字节
  start: 0,
  end: 10,
});


// 每读取一次数据，就会触发一次 data 事件
// 参数 chunk 是 这次读取到的数据，类型为 Buffer
stream.on('data', (chunk) => {
  console.log(chunk.toString())

  // 暂停读取
  stream.pause()

  // 1 秒后恢复读取
  setTimeout(() => {
    stream.resume()
  }, 1000)
});

// 读取完成时触发
stream.on('end', () => {
  console.log('读取完成')
});
```

```js
import fs from 'fs';

const stream = fs.createReadStream('./data.txt', {
  highWaterMark: 3, // 每次最多读取 3 个字节
  start: 0,
  end: 10,
});

stream.on('open', fd => {
  console.log('当打开文件时触发')
  // 参数是 文件描述符
  // 流在fs模块，不在fs/promises模块 => fd 是数字，不是句柄
  console.log(fd)
})

stream.on('data', (chunk) => {
  console.log('当流读取数据时触发')
  // 参数是每次读取到的内容 对应的 Buffer 对象
  console.log(chunk.toString())
});

// 读取完成时触发
stream.on('end', () => {
  console.log('当流读取完成时触发')
});

// 关闭文件时触发
// 一般读取完成时，会触发 close 事件
// 我们也可以通过 stream.close() 手动关闭
stream.on('close', () => {
  console.log('当关闭文件时触发')
})
```

```js
import fs from 'fs';

const stream = fs.createReadStream('./data.txt', {
  highWaterMark: 3, // 每次最多读取 3 个字节
  start: 0,
  end: 10,
});


stream.on('data', (chunk) => {
  console.log(chunk.toString())
  stream.pause()
  setTimeout(() => {
    stream.resume()
  }, 1000)
});

// 当流暂停时触发
stream.on('pause', () => {
  console.log('当流暂停时触发')
})

// 当流恢复时触发
stream.on('resume', () => {
  console.log('当流恢复时触发')
})
```

